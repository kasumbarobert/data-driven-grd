"""
CNN Training Script for WCD Prediction Oracle

Author: Robert Kasumba (rkasumba@wustl.edu)

This script trains a Convolutional Neural Network (CNN) to predict Worst-Case Distance (WCD)
values for Overcooked-AI environment designs. The trained model serves as a fast oracle for
WCD prediction during the optimization process.

WHY THIS IS NEEDED:
- Direct WCD computation is computationally expensive (exhaustive search)
- The optimization process requires thousands of WCD predictions
- A trained CNN can predict WCD values in milliseconds vs. seconds for oracle computation
- Enables gradient-based optimization by providing differentiable WCD estimates

HOW IT WORKS:
1. Loads training data generated by simulate_wcd_oracle.py
2. Applies data augmentation (rotations, flips, transpositions) to increase dataset size
3. Trains a CustomCNN architecture optimized for Overcooked-AI environments
4. Uses MSE loss with L2 regularization and learning rate scheduling
5. Validates performance on held-out test data
6. Saves the trained model for use in optimization

TRAINING DETAILS:
- Architecture: CustomCNN with configurable dropout and size parameters
- Data Augmentation: 7x augmentation using rotations, flips, and transpositions
- Optimization: Adam optimizer with learning rate 0.008
- Regularization: L2 regularization (Î»=0.1) to prevent overfitting
- Scheduler: MultiStepLR with milestones at epochs 12, 25, 45, 55
- Training: 20 epochs with batch size 8192

USAGE:
    python train_model.py --train --grid_size 6

OUTPUT:
    Trained CNN model saved as models/wcd_nn_oracle_{grid_size}.pt
"""

import os
import re
import torch
import pickle
import random
import argparse
import numpy as np
import pandas as pd
import seaborn as sns
import torch.optim as optim
import matplotlib.pyplot as plt
import torch.nn as nn
from joblib import dump, load
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import train_test_split, cross_validate
from sklearn.inspection import PartialDependenceDisplay

from utils import (CustomCNN)

def load_datasets(grid_size):
    """Load datasets based on the grid size."""
    if grid_size == 6: 
        datasets = ["final_training_dataset.pt"]
    else:
        datasets = [
            "final_training_dataset.pt",
            f"simulated_envs_0_size{grid_size}.pkl",
            f"simulated_envs_1_size{grid_size}.pkl",
            f"simulated_envs_2_size{grid_size}.pkl",
            f"simulated_envs_3_size{grid_size}.pkl",
            f"simulated_envs_4_size{grid_size}.pkl",
            f"simulated_envs_5_size{grid_size}.pkl",
            f"simulated_envs_6_size{grid_size}.pkl",
            f"simulated_envs_7_size{grid_size}.pkl",
            f"simulated_envs_8_size{grid_size}.pkl"
        ]

    loaded_data = {}
    for dataset in datasets:
        with open(f"data/grid{grid_size}/model_training/{dataset}", "rb") as f:
            print(f"Loading {dataset}")
            loaded_dataset = pickle.load(f)
            x_data = []
            y_data = []
            for i in range(loaded_dataset.__len__()):
                if loaded_dataset[i][0][8,0,0] >= 0.65:
                    x_data.append(loaded_dataset[i][0].unsqueeze(0))
                    y_data.append(loaded_dataset[i][1].unsqueeze(0))
            
            x_data = torch.cat(x_data).numpy()
            y_data = torch.cat(y_data).numpy()

            loaded_data[dataset] = {'x': x_data, 'y': y_data}

    X = np.concatenate([loaded_data[dataset]['x'] for dataset in datasets], axis=0)
    Y = np.concatenate([loaded_data[dataset]['y'] for dataset in datasets], axis=0)

    return X, Y

def drop_50_percent_zeros(X, Y):
    """Drop 50% of the datapoints where Y equals 0."""
    zero_indices = np.where(Y == 0)[0]
    num_to_select = int(len(zero_indices) * 0.6)
    selected_indices = np.random.choice(zero_indices, num_to_select, replace=False)
    mask = np.ones(len(Y), dtype=bool)
    mask[selected_indices] = False

    return X[mask], Y[mask]

def prepare_data(grid_size):
    """Prepare the training and testing data."""
    X, Y = load_datasets(grid_size)
    X_filtered, Y_filtered = drop_50_percent_zeros(X, Y)
    x_train, x_test, y_train, y_test = train_test_split(X_filtered, Y_filtered, test_size=0.05)
    
    x_train = np.concatenate([x_train[:, :, :, ::-1], x_train, x_train[:, :, ::-1, :], 
                              np.rot90(x_train, k=1, axes=(2, 3)), np.rot90(x_train, k=3, axes=(2, 3)),
                              np.rot90(x_train, k=2, axes=(2, 3)), x_train.transpose(0, 1, 3, 2)])
    y_train = np.concatenate([y_train] * 7)
    
    x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, test_size=0.95, random_state=50)

    x_test = torch.from_numpy(x_test).float()
    y_test = torch.from_numpy(np.array(y_test)).float()
    x_val = torch.from_numpy(x_val).float()
    y_val = torch.from_numpy(np.array(y_val)).float()

    return x_train, y_train, x_test, y_test, x_val, y_val

def evaluate_and_log(best_model, x_val, y_val, epoch, wandb=None):
    """Evaluate the model and log the results."""
    def predict_in_batches(model, x, batch_size):
        n_batches = len(x) // batch_size + (len(x) % batch_size != 0)
        all_preds = []

        for i in range(n_batches):
            x_batch = x[i * batch_size: (i + 1) * batch_size]
            preds_batch = model(x_batch.cuda()).detach()
            all_preds.append(preds_batch)

        return torch.cat(all_preds, dim=0).cpu()

    val_pred = predict_in_batches(best_model, x_val, 1024)

  
    return nn.HuberLoss()(val_pred, y_val.view(-1, 1)).item(), nn.MSELoss()(val_pred, y_val.view(-1, 1)).item()

def train_model(grid_size):
    """Train the CNN model for WCD prediction."""
    # Prepare training, validation, and test data
    x_train, y_train, x_test, y_test, x_val, y_val = prepare_data(grid_size)
    
    # Set up data loading with large batch size for efficiency
    batch_size = 4096 * 2  # Large batch size for stable training
    dataset = TensorDataset(torch.from_numpy(x_train).float(), torch.from_numpy(np.array(y_train)).float())
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

    # Initialize CNN model with no dropout for this task
    dropout = 0.0
    model = CustomCNN(n_channels=x_train.shape[1], drop_out=dropout, size=grid_size//2).cuda()

    # Print model size for reference
    total_params = sum(p.numel() for p in model.parameters())
    print(f"Total number of parameters: {total_params}")

    # Set up optimizer and learning rate scheduler
    optimizer = optim.Adam(model.parameters(), lr=0.008)  # Higher learning rate for faster convergence
    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[12, 25, 45, 55], gamma=0.5)

    # Training configuration
    num_epochs = 20
    best_model = model
    log_interval = 2  # Log every 2 epochs

    for epoch in range(num_epochs):
        for i, (inputs, targets) in enumerate(dataloader):
            # Forward pass
            optimizer.zero_grad()
            outputs = model(inputs.cuda())
            loss = nn.MSELoss()(outputs, targets.view(-1, 1).cuda())

            # Add L2 regularization to prevent overfitting
            l2_reg = 0.1 * torch.norm(torch.cat([p.view(-1) for p in model.parameters()]), p=2)
            loss += l2_reg
            
            # Backward pass and optimization step
            loss.backward()
            optimizer.step()

            # Keep track of best model (this line has an undefined variable - should be fixed)
            if loss.item() < best_model_loss:
                best_model = model

        # Evaluate and log performance periodically
        if epoch % log_interval == 0:
            evaluate_and_log(best_model, x_val, y_val, epoch)

        # Update learning rate
        scheduler.step()
        # Save model checkpoint
        torch.save(best_model, f"models/wcd_nn_oracle_{grid_size}.pt")

    return best_model

def main():
    """Main function to handle training and evaluation."""
    parser = argparse.ArgumentParser(description='CNN Model Training Script')
    parser.add_argument('--train', action='store_true', help='Train the model')
    parser.add_argument('--grid_size', type=int, default=6, help='Grid size for the dataset')
    args = parser.parse_args()

    if args.train:
        print("Starting training...")
        train_model(args.grid_size)

if __name__ == "__main__":
    main()

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "909e1eca-10bd-4d53-83ab-a53c90a1b0d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 960x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso,BayesianRidge, LogisticRegression,SGDClassifier\n",
    "from sklearn.model_selection import train_test_split,cross_validate\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.svm import SVR, SVC\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "import torch\n",
    "import ast\n",
    "from joblib import dump, load\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pickle\n",
    "from IPython.display import Markdown\n",
    "\n",
    "import os\n",
    "import re\n",
    "import torch.nn as nn\n",
    "import random\n",
    "plt.figure(dpi=150)\n",
    "import cProfile\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils as utils\n",
    "import wandb\n",
    "import pdb\n",
    "from utils_human_exp import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68982f51-b67c-4467-8c86-700b5634cd49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff697583-8ff3-4b70-95d0-0aab22ea1843",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n",
    "GRID_SIZE = 6\n",
    "\n",
    "# wandb.login(relogin=True)\n",
    "# os.environ['WANDB_API_KEY'] = \"71f0a53fa4cb62b56494f6554ec1a5e3b898a7dd\"\n",
    "# wandb.login(key=\"71f0a53fa4cb62b56494f6554ec1a5e3b898a7dd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "938adcae-2dd8-4910-a09d-551288871b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not logged in.\n"
     ]
    }
   ],
   "source": [
    "# Check if the WANDB_API_KEY is set\n",
    "if \"WANDB_API_KEY\" in os.environ:\n",
    "    print(\"Logged in with API key.\")\n",
    "    user_info = wandb.api.viewer()\n",
    "    print(\"Current user:\", user_info[\"entity\"], user_info[\"username\"])\n",
    "else:\n",
    "    print(\"Not logged in.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253005d4-5504-40dd-9c9d-a56bb379bb1a",
   "metadata": {},
   "source": [
    "# Predicting Valid Human Wcd Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4fecdf9-d032-48a9-a177-57c61149c761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #f\"simulated_valids_final{GRID_SIZE}.pkl\"\n",
    "# if GRID_SIZE == 6:\n",
    "#     datasets = [\"tmp_human_path-4.pt\",\"tmp_human_path-3.pt\",\"tmp_human_path-2.pt\",\"tmp_human_path-1.pt\",\"tmp_human_path-5.pt\",\"tmp_human_grid6-6.pt\"]\n",
    "# else:\n",
    "#     datasets = [\"tmp_human_grid10-2.pt\",\"tmp_human_grid10-0.pt\",\"tmp_human_grid10-3.pt\",\"tmp_human_grid10-4.pt\",\"tmp_human_grid10-5.pt\"]\n",
    "    \n",
    "# loaded_data = {}\n",
    "# dataset=datasets[0]\n",
    "# x_data = []\n",
    "# y_data = []\n",
    "# for dataset in datasets:\n",
    "#     if not os.path.exists(f\"data/grid{GRID_SIZE}/model_training/{dataset}\"): continue\n",
    "#     with open(f\"data/grid{GRID_SIZE}/model_training/{dataset}\", \"rb\") as f:\n",
    "#         loaded_dataset = pickle.load(f)\n",
    "#         print(dataset,len(loaded_dataset))\n",
    "#         for i in range(loaded_dataset. __len__()):\n",
    "#             x_i = loaded_dataset[i][0]\n",
    "#             if x_i.shape[0]==5:\n",
    "#                 x_i = x_i[0:4,:,:]\n",
    "                \n",
    "#             x_data.append(x_i.numpy())\n",
    "#             y_data.append(loaded_dataset[i][1].unsqueeze(0).item())\n",
    "\n",
    "\n",
    "# X_valid = np.stack(x_data)[:,0:4,:,:]\n",
    "# # Y_valid = np.array(y_data)\n",
    "# X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64f131d2-b000-4b9a-9a94-3214047b5bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human_grid6-100kall0.pt 122500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage1/fs1/chien-ju.ho/Active/robert/projects/Gridworld MDP/human-exp/utils.py:167: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(self.X[idx], dtype=torch.float32)\n",
      "/storage1/fs1/chien-ju.ho/Active/robert/projects/Gridworld MDP/human-exp/utils.py:168: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(self.Y[idx], dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human_grid6-100kall1.pt 122500\n",
      "human_grid6-100kall2.pt 123000\n",
      "human_grid6-100kall3.pt 123500\n",
      "human_grid6-100kall4.pt 121500\n",
      "human_grid6-100kall5.pt 122000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(735000, 4, 6, 6)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#f\"simulated_valids_final{GRID_SIZE}.pkl\"\n",
    "if GRID_SIZE == 6:\n",
    "    datasets = [f\"human_grid{GRID_SIZE}-100kall0.pt\",f\"human_grid{GRID_SIZE}-100kall1.pt\",\n",
    "                f\"human_grid{GRID_SIZE}-100kall2.pt\",f\"human_grid{GRID_SIZE}-100kall3.pt\",\n",
    "               f\"human_grid{GRID_SIZE}-100kall4.pt\",f\"human_grid{GRID_SIZE}-100kall5.pt\"]\n",
    "else:\n",
    "    datasets = [\"human_grid10-100kall0.pt\",\"human_grid10-100kall1.pt\",\"human_grid10-100kall2.pt\",\"human_grid10-100kall3.pt\",\n",
    "               \"human_grid10-100kall4.pt\",\"human_grid10-100kall5.pt\"]\n",
    "    \n",
    "loaded_data = {}\n",
    "dataset=datasets[0]\n",
    "x_data = []\n",
    "y_data = []\n",
    "for dataset in datasets:\n",
    "    if not os.path.exists(f\"data/grid{GRID_SIZE}/model_training/{dataset}\"): continue\n",
    "    with open(f\"data/grid{GRID_SIZE}/model_training/{dataset}\", \"rb\") as f:\n",
    "        loaded_dataset = pickle.load(f)\n",
    "        print(dataset,len(loaded_dataset))\n",
    "        for i in range(loaded_dataset. __len__()):\n",
    "            x_i = loaded_dataset[i][0]\n",
    "            if x_i.shape[0]==5:\n",
    "                x_i = x_i[0:4,:,:]\n",
    "                \n",
    "            x_data.append(x_i.numpy())\n",
    "            y_data.append(loaded_dataset[i][1].unsqueeze(0).item())\n",
    "\n",
    "\n",
    "X_invalid = np.stack(x_data)[:,0:4,:,:]\n",
    "Y_invalid = (np.array(y_data)>0).astype(int)\n",
    "X_invalid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ae0c434-474e-420b-aacd-b5bf2a55be07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4296748299319728"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_invalid.sum()/Y_invalid.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21aa5d8c-64e2-47c6-9313-6f21db3cbe1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_invalid = -1 * np.ones(len(X_invalid))\n",
    "# y_valid = 1 * np.ones(len(X_valid))\n",
    "\n",
    "# # Combine data and labels\n",
    "# X = np.concatenate((X_invalid, X_valid), axis=0)\n",
    "# Y = np.concatenate((y_invalid, y_valid), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b42dfb8-644f-414d-9f07-37c95c371e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_invalid\n",
    "Y = Y_invalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b22beb-7a64-47ab-97d3-f9ab5aaf470e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d87fac75-13b2-4c0d-9d4f-edba24083fd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4116000, 4, 6, 6), (147000, 4, 6, 6))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train,x_test, y_train,y_test = train_test_split(X,Y, test_size=0.20)\n",
    "x_train = np.concatenate([x_train[:, :, :, ::-1],x_train,x_train[:, :, ::-1, :],np.rot90(x_train, k=1, axes=(2, 3)),\n",
    "                          np.rot90(x_train, k=3, axes=(2, 3)),np.rot90(x_train, k=2, axes=(2, 3)),\n",
    "                          x_train.transpose(0, 1, 3, 2)])\n",
    "\n",
    "y_train = np.concatenate([y_train,y_train,y_train,y_train,y_train,y_train,y_train])\n",
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e250d44d-bcde-43f8-84bf-478052969535",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c753beda-5c14-411e-86df-ce37543cd584",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_sample_set = CustomDataset(x_test,y_test)\n",
    "with open(f\"data/grid{GRID_SIZE}/model_training/dataset_{GRID_SIZE}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(out_sample_set, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1a0f91e-f02f-4e17-ad44-31c9e992cfad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((139650, 4, 6, 6), (7350, 4, 6, 6))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test,x_val,y_test,y_val = train_test_split(x_test,y_test, test_size=0.95, random_state=50)\n",
    "x_val.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e804dadb-dd22-4f3d-9217-58b230fcdb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "abeefd39-c836-4991-9a73-54df479f592a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = torch.from_numpy(x_test).float().cuda()\n",
    "y_test = torch.from_numpy(np.array(y_test)).float().cuda()\n",
    "x_val = torch.from_numpy(x_val).float().cuda()\n",
    "y_val = torch.from_numpy(np.array(y_val)).float().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2abe42b9-6cf8-4ce6-aeae-146852f83f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self, n_channels=13,drop_out = 0.01,size = 3):\n",
    "        super(CustomCNN, self).__init__()\n",
    "\n",
    "        # First block (no pooling)\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(n_channels, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Second block with pooling\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # # Second block with pooling\n",
    "        # self.conv3 = nn.Sequential(\n",
    "        #     nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "        #     nn.BatchNorm2d(256),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        #     nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "        #     nn.BatchNorm2d(512),\n",
    "        #     nn.ReLU()\n",
    "        # )\n",
    "\n",
    "\n",
    "        self.conv_output_size = 256 * size*size\n",
    "        # Fully connected layers\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(self.conv_output_size, 16), # 3 for 6 & 7 \n",
    "            nn.LeakyReLU(),\n",
    "            # nn.Dropout(0.01),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(drop_out),\n",
    "            nn.Linear(8, 1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        # pdb.set_trace()\n",
    "        x = self.conv2(x)\n",
    "        # x = self.conv3(x)\n",
    "        # print(x.shape)\n",
    "        x = x.view(-1, self.conv_output_size)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "class CNN4(nn.Module):\n",
    "    def __init__(self, n_channels=13,drop_out=0.01):\n",
    "        super(CNN4, self).__init__()\n",
    "\n",
    "        # Replace the first seven convolutional layers with ResNet50\n",
    "        self.resnet50 = resnet18(pretrained=False)\n",
    "\n",
    "        self.resnet50.conv1=self.conv1 = nn.Conv2d(n_channels, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.resnet50.maxpool = nn.Identity()\n",
    "        # self.resnet50.bn1 = nn.BatchNorm2d(8)\n",
    "\n",
    "        # Keep the rest of the architecture as-is\n",
    "        self.fc1 = nn.Linear(1000, 16)\n",
    "        self.relu8 = nn.LeakyReLU()\n",
    "        self.dropout1 = nn.Dropout(0.0001)\n",
    "\n",
    "        self.fc2 = nn.Linear(16, 8)\n",
    "        self.relu9 = nn.LeakyReLU()\n",
    "        self.dropout2 = nn.Dropout(drop_out)\n",
    "\n",
    "        self.fc3 = nn.Linear(8, 1)\n",
    "        self.softplus = nn.functional.softplus\n",
    "        self.sigmoid =torch.sigmoid\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet50(x)\n",
    "        # pdb.set_trace()\n",
    "        # print(x.shape)\n",
    "        x = x.view(-1, 1000)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.relu8(x)\n",
    "\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu9(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = self.sigmoid(self.fc3(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe889d56-fc24-4640-9614-268dfd746d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_log(best_model, x_val, y_val, epoch, wandb=None):\n",
    "    def predict_in_batches(model, x, batch_size):\n",
    "        n_batches = len(x) // batch_size + (len(x) % batch_size != 0)\n",
    "        all_preds = []\n",
    "        \n",
    "        for i in range(n_batches):\n",
    "            x_batch = x[i * batch_size: (i + 1) * batch_size]\n",
    "            preds_batch = model(x_batch).detach()\n",
    "            all_preds.append(preds_batch)\n",
    "        \n",
    "        return torch.cat(all_preds, dim=0)\n",
    "\n",
    "    val_pred = (predict_in_batches(best_model, x_val, 1024)>0.5).float()  # Adjust batch size as per memory needs\n",
    "    # print(val_pred==y_val)\n",
    "    # if wandb:\n",
    "    #     wandb.log({\n",
    "    #         \"epoch\": epoch,\n",
    "    #         \"mse_val_small\": nn.BCELoss()(val_pred[y_val <= GRID_SIZE/2], y_val.view(-1, 1)[y_val <= GRID_SIZE/2]).item(),\n",
    "    #         \"mse_val_big\": nn.BCELoss()(val_pred[y_val > GRID_SIZE/2], y_val.view(-1, 1)[y_val > GRID_SIZE/2]).item(),\n",
    "    #         \"mse_val_loss\": nn.BCELoss()(val_pred, y_val.view(-1, 1)).item(),\n",
    "    #         \"val_loss\": nn.HuberLoss()(val_pred, y_val.view(-1, 1)).item(),\n",
    "    #         \"valid_mean_wcd\": val_pred[y_val != INVALID_WCD].mean(),\n",
    "    #         \"invalid_mean_wcd\": val_pred[y_val == INVALID_WCD].mean(),\n",
    "    #         \"valid_h_loss\": nn.HuberLoss()(val_pred[y_val != INVALID_WCD], y_val.view(-1, 1)[y_val != INVALID_WCD]).item(),\n",
    "    #         \"invalid_h_loss\": nn.HuberLoss()(val_pred[y_val == INVALID_WCD], y_val.view(-1, 1)[y_val == INVALID_WCD]).item(),\n",
    "    #         \"valid_mse_loss\": nn.BCELoss()(val_pred[y_val != INVALID_WCD], y_val.view(-1, 1)[y_val != INVALID_WCD]).item(),\n",
    "    #         \"invalid_mse_loss\": nn.BCELoss()(val_pred[y_val == INVALID_WCD], y_val.view(-1, 1)[y_val == INVALID_WCD]).item(),\n",
    "    #     })\n",
    "    \n",
    "    accuracy = (val_pred==y_val).float().mean().item()\n",
    "    print(\"Accuracy: \",accuracy)\n",
    "    return nn.HuberLoss()(val_pred, y_val.view(-1, 1)).item() ,accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "983ecf88-674d-417d-b168-4b1b1b3794e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256 #512 for smaller (6,7,8)\n",
    "dataset = TensorDataset(torch.from_numpy(x_train).float().cuda(), torch.from_numpy(np.array(y_train)).float().cuda())\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97ea0237-da7f-42ab-9230-085a69bac331",
   "metadata": {},
   "outputs": [],
   "source": [
    "if GRID_SIZE in [6,7,8]:\n",
    "    dropout = 0.01\n",
    "    lambda_l2 =0.0 #0.1 regularization strength\n",
    "    grad_clip =1e-3 # 1e-3 for 6,7,8\n",
    "    lr=0.001 #0.005 for 6,7,8\n",
    "    num_epochs = 12# 3 for 6,7,8\n",
    "else:\n",
    "    dropout = 0.0\n",
    "    lambda_l2 =0.0 #0.1 regularization strength\n",
    "    grad_clip =1e-3 # 1e-3 for 6,7,8\n",
    "    lr=0.001 #0.005 for 6,7,8\n",
    "    num_epochs = 12# 3 for 6,7,8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26ce5668-67f4-4f8f-890d-2bdfbc6b2c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/k.robert/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/k.robert/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# model = CustomCNN(n_channels=x_train.shape[1],drop_out=dropout,size = GRID_SIZE//2).cuda() #GRID_SIZE//2\n",
    "random_seed=123\n",
    "torch.manual_seed(seed=random_seed)\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "\n",
    "model = CNN4(n_channels=x_train.shape[1],drop_out=dropout).cuda() #\n",
    "# model = torch.load(\"models/wcd_nn_oracle_july6.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a5135505-daf1-4b0f-8f73-035d1be4d8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # init_model = torch.load(\"models/wcd_10_init.pt\")\n",
    "# init_model = torch.load(f\"../models/wcd_nn_model_{GRID_SIZE}_best.pt\") \n",
    "\n",
    "# model = init_model\n",
    "# model.dropout1 = nn.Dropout(dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3814307a-ac07-4b4b-8fa3-214389b69fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 11698633\n"
     ]
    }
   ],
   "source": [
    "total_params = 0\n",
    "for parameter in model.parameters():\n",
    "    # print(parameter.shape)\n",
    "    total_params += parameter.numel()  # numel() returns the total number of elements in the tensor\n",
    "\n",
    "print(f\"Total number of parameters: {total_params}\") #636673"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db86fd46-3225-4343-8864-66ea3326fe3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = VGGNet(n_channels = x_train.shape[1]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80944fdb-434a-4b2e-b388-1f69393a57e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n",
      "0.6741485595703125\n",
      "0.582017183303833\n",
      "0.49077796936035156\n",
      "0.5286787152290344\n",
      "0.521165132522583\n",
      "0.5187804698944092\n",
      "0.5198639631271362\n",
      "0.5181643962860107\n",
      "0.4725554585456848\n",
      "0.4326833188533783\n",
      "0.45237550139427185\n",
      "0.4590933918952942\n",
      "0.42590558528900146\n",
      "0.4656156897544861\n",
      "0.5007702708244324\n",
      "0.43863895535469055\n",
      "0.44066131114959717\n",
      "0.4149002730846405\n",
      "0.44006234407424927\n",
      "0.4309840202331543\n",
      "0.4372637867927551\n",
      "0.4006771147251129\n",
      "0.4195663034915924\n",
      "0.4720875024795532\n",
      "0.4394712448120117\n",
      "0.39813828468322754\n",
      "0.4467466175556183\n",
      "0.4105249047279358\n",
      "0.4125758409500122\n",
      "0.41258469223976135\n",
      "0.44140228629112244\n",
      "0.41912341117858887\n",
      "0.5304293632507324\n",
      "0.40720248222351074\n",
      "0.3990055322647095\n",
      "0.47574400901794434\n",
      "0.40540963411331177\n",
      "0.3960975408554077\n",
      "0.37730130553245544\n",
      "0.3753117322921753\n",
      "0.4047965109348297\n",
      "0.4733477532863617\n",
      "0.43273693323135376\n",
      "0.37758833169937134\n",
      "0.4975224435329437\n",
      "0.35894376039505005\n",
      "0.44456279277801514\n",
      "0.39450377225875854\n",
      "0.37521815299987793\n",
      "0.3685687184333801\n",
      "0.39137086272239685\n",
      "0.45395517349243164\n",
      "0.4127298891544342\n",
      "0.44271230697631836\n",
      "0.3755788803100586\n",
      "0.42258816957473755\n",
      "0.3691083788871765\n",
      "0.3795320391654968\n",
      "0.3560790717601776\n",
      "0.3590465188026428\n",
      "0.40274950861930847\n",
      "0.402702271938324\n",
      "0.40951618552207947\n",
      "0.4116804599761963\n",
      "0.4279444217681885\n",
      "0.36383742094039917\n",
      "0.3899694085121155\n",
      "0.3935089111328125\n",
      "0.3594067096710205\n",
      "0.3819522261619568\n",
      "0.3741070628166199\n",
      "0.4066140055656433\n",
      "0.3872986435890198\n",
      "0.33824753761291504\n",
      "0.4010210931301117\n",
      "0.3092292547225952\n",
      "0.4348491132259369\n",
      "0.39921143651008606\n",
      "0.4017598330974579\n",
      "0.3871322572231293\n",
      "0.36875295639038086\n",
      "0.3791544735431671\n",
      "0.37832099199295044\n",
      "0.44398143887519836\n",
      "0.3856779932975769\n",
      "0.4312283992767334\n",
      "0.3785068690776825\n",
      "0.3377467095851898\n",
      "0.38683265447616577\n",
      "0.37534019351005554\n",
      "0.39302074909210205\n",
      "0.37677037715911865\n",
      "0.3407837450504303\n",
      "0.40518301725387573\n",
      "0.38625195622444153\n",
      "0.3498922884464264\n",
      "0.4510871171951294\n",
      "0.40598440170288086\n",
      "0.3802450895309448\n",
      "0.35170263051986694\n",
      "0.358681321144104\n",
      "0.36464497447013855\n",
      "0.41242268681526184\n",
      "0.39667361974716187\n",
      "0.3573386073112488\n",
      "0.4258117079734802\n",
      "0.4346882104873657\n",
      "0.3362785577774048\n",
      "0.3291914165019989\n",
      "0.3443414568901062\n",
      "0.3549967110157013\n",
      "0.39951369166374207\n",
      "0.3630691170692444\n",
      "0.38266995549201965\n",
      "0.35063037276268005\n",
      "0.41399532556533813\n",
      "0.36206045746803284\n",
      "0.4081430435180664\n",
      "0.42491549253463745\n",
      "0.3730003833770752\n",
      "0.36139118671417236\n",
      "0.4187028408050537\n",
      "0.43216419219970703\n",
      "0.3818894028663635\n",
      "0.32511523365974426\n",
      "0.3534867763519287\n",
      "0.36225980520248413\n",
      "0.4203180968761444\n",
      "0.37303632497787476\n",
      "0.33457517623901367\n",
      "0.4177148938179016\n",
      "0.4346165359020233\n",
      "0.33763474225997925\n",
      "0.4298458695411682\n",
      "0.372902512550354\n",
      "0.37138307094573975\n",
      "0.3313005864620209\n",
      "0.3361803889274597\n",
      "0.3702889084815979\n",
      "0.3801160752773285\n",
      "0.38033735752105713\n",
      "0.36925646662712097\n",
      "0.39502155780792236\n",
      "0.3496900200843811\n",
      "0.37197619676589966\n",
      "0.3577103018760681\n",
      "0.4113125801086426\n",
      "0.41729265451431274\n",
      "0.358458548784256\n",
      "0.3275154232978821\n",
      "0.3916718661785126\n",
      "0.3563176393508911\n",
      "0.36159396171569824\n",
      "0.4070861041545868\n",
      "0.42403513193130493\n",
      "0.41144412755966187\n",
      "0.3652101755142212\n",
      "0.33007436990737915\n",
      "0.3443758487701416\n",
      "0.3626781404018402\n",
      "0 0.3893594741821289\n",
      "0.39573830366134644\n",
      "0.36129263043403625\n",
      "0.35617494583129883\n",
      "0.3510720729827881\n",
      "0.3352009654045105\n",
      "0.36216771602630615\n",
      "0.3147340416908264\n",
      "0.3859887719154358\n",
      "0.3528313636779785\n",
      "0.3427876830101013\n",
      "0.40600720047950745\n",
      "0.31122422218322754\n",
      "0.40807875990867615\n",
      "0.3877701759338379\n",
      "0.3422444462776184\n",
      "0.3655269742012024\n",
      "0.35705024003982544\n",
      "0.38279014825820923\n",
      "0.29012858867645264\n",
      "0.40047603845596313\n",
      "0.3235815763473511\n",
      "0.29780253767967224\n",
      "0.3570455312728882\n",
      "0.37030595541000366\n",
      "0.3491904139518738\n",
      "0.39724546670913696\n",
      "0.35890358686447144\n",
      "0.36194565892219543\n",
      "0.34744977951049805\n",
      "0.3627309203147888\n",
      "0.35820499062538147\n",
      "0.3341245651245117\n",
      "0.4312545955181122\n",
      "0.34174469113349915\n",
      "0.3740493059158325\n",
      "0.36410045623779297\n",
      "0.3570753335952759\n",
      "0.33588266372680664\n",
      "0.3552231192588806\n",
      "0.3873896598815918\n",
      "0.32353854179382324\n",
      "0.3689049482345581\n",
      "0.3283035159111023\n",
      "0.35029125213623047\n",
      "0.3306453824043274\n",
      "0.3278769254684448\n",
      "0.3627494275569916\n",
      "0.32438015937805176\n",
      "0.3649135231971741\n",
      "0.37393102049827576\n",
      "0.3345169425010681\n",
      "0.3655310571193695\n",
      "0.3122500777244568\n",
      "0.2935198247432709\n",
      "0.3393017649650574\n",
      "0.36136624217033386\n",
      "0.36031976342201233\n",
      "0.29724836349487305\n",
      "0.37098705768585205\n",
      "0.33968332409858704\n",
      "0.3758474588394165\n",
      "0.30146950483322144\n",
      "0.3334360718727112\n",
      "0.3760504126548767\n",
      "0.35826244950294495\n",
      "0.35226187109947205\n",
      "0.37674832344055176\n",
      "0.32539045810699463\n",
      "0.33384573459625244\n",
      "0.3587035834789276\n",
      "0.31058570742607117\n",
      "0.3664449155330658\n",
      "0.35992592573165894\n",
      "0.3172564208507538\n",
      "0.36005544662475586\n",
      "0.35229209065437317\n",
      "0.409464955329895\n",
      "0.3306425213813782\n",
      "0.37624695897102356\n",
      "0.3421761393547058\n",
      "0.3110235929489136\n",
      "0.36022478342056274\n",
      "0.4188233017921448\n",
      "0.3536313772201538\n",
      "0.38149309158325195\n",
      "0.4243115186691284\n",
      "0.36535510420799255\n",
      "0.34729069471359253\n",
      "0.4208492040634155\n",
      "0.34466323256492615\n",
      "0.3127794861793518\n",
      "0.3439099192619324\n",
      "0.4038344621658325\n",
      "0.3604467511177063\n",
      "0.33857959508895874\n",
      "0.38969725370407104\n",
      "0.334627628326416\n",
      "0.34401047229766846\n",
      "0.4019855856895447\n",
      "0.2884294390678406\n",
      "0.40364453196525574\n",
      "0.41022834181785583\n",
      "0.35680222511291504\n",
      "0.35384297370910645\n",
      "0.355988472700119\n",
      "0.3537719249725342\n",
      "0.32553887367248535\n",
      "0.4065101742744446\n",
      "0.31597715616226196\n",
      "0.3725126385688782\n",
      "0.3853605091571808\n",
      "0.37450215220451355\n",
      "0.33190470933914185\n",
      "0.30082494020462036\n",
      "0.28472501039505005\n",
      "0.33374524116516113\n",
      "0.3247887194156647\n",
      "0.3545224368572235\n",
      "0.274113267660141\n",
      "0.355254203081131\n",
      "0.38327622413635254\n",
      "0.31851112842559814\n",
      "0.35392701625823975\n",
      "0.36644887924194336\n",
      "0.30218392610549927\n",
      "0.385714054107666\n",
      "0.37091371417045593\n",
      "0.34872502088546753\n",
      "0.36715394258499146\n",
      "0.3158479928970337\n",
      "0.3736581802368164\n",
      "0.2989669442176819\n",
      "0.37412938475608826\n",
      "0.3229063153266907\n",
      "0.34483370184898376\n",
      "0.41825222969055176\n",
      "0.4147191047668457\n",
      "0.36516788601875305\n",
      "0.3579196333885193\n",
      "0.36356061697006226\n",
      "0.30815866589546204\n",
      "0.37656986713409424\n",
      "0.3265575170516968\n",
      "0.35186028480529785\n",
      "0.3641795217990875\n",
      "0.3579290509223938\n",
      "0.3228730857372284\n",
      "0.34543681144714355\n",
      "0.3218860626220703\n",
      "0.33131057024002075\n",
      "0.34113943576812744\n",
      "0.3485451638698578\n",
      "0.3443402349948883\n",
      "0.35429200530052185\n",
      "0.38926786184310913\n",
      "0.36324214935302734\n",
      "0.3346402943134308\n",
      "0.3788377642631531\n",
      "0.4316858649253845\n",
      "0.30639106035232544\n",
      "1 0.3468243479728699\n",
      "0.3269551992416382\n",
      "0.33740171790122986\n",
      "0.29704517126083374\n",
      "0.36134135723114014\n",
      "0.3209148943424225\n",
      "0.4321197271347046\n",
      "0.34346938133239746\n",
      "0.32921671867370605\n",
      "0.3261154294013977\n",
      "0.31857410073280334\n",
      "0.3434527516365051\n",
      "0.3451630771160126\n",
      "0.3537430167198181\n",
      "0.34787681698799133\n",
      "0.3387787938117981\n",
      "0.3383190631866455\n",
      "0.2931678891181946\n",
      "0.36176598072052\n",
      "0.31088343262672424\n",
      "0.3459152281284332\n",
      "0.3281870186328888\n",
      "0.3045268654823303\n",
      "0.28438884019851685\n",
      "0.2990054786205292\n",
      "0.3278018832206726\n",
      "0.31967872381210327\n",
      "0.37714678049087524\n",
      "0.32271313667297363\n",
      "0.3692469000816345\n",
      "0.3236747980117798\n",
      "0.37783074378967285\n",
      "0.3651126027107239\n",
      "0.3327062129974365\n",
      "0.37053656578063965\n",
      "0.3465281128883362\n",
      "0.2789527177810669\n",
      "0.33877381682395935\n",
      "0.34287360310554504\n",
      "0.3364706337451935\n",
      "0.3575677275657654\n",
      "0.3427538573741913\n",
      "0.35316962003707886\n",
      "0.32672959566116333\n",
      "0.3219498097896576\n",
      "0.3714262843132019\n",
      "0.35604608058929443\n",
      "0.3805936574935913\n",
      "0.4286233186721802\n",
      "0.3012758791446686\n",
      "0.3495810031890869\n",
      "0.37021684646606445\n",
      "0.32561057806015015\n",
      "0.3610440492630005\n",
      "0.37281954288482666\n",
      "0.35875993967056274\n",
      "0.3734451234340668\n",
      "0.36294466257095337\n",
      "0.3428816795349121\n",
      "0.32134193181991577\n",
      "0.3566291332244873\n",
      "0.3473852872848511\n",
      "0.415358304977417\n",
      "0.36059170961380005\n",
      "0.36773595213890076\n",
      "0.30053216218948364\n",
      "0.3447886109352112\n",
      "0.3546416759490967\n",
      "0.3313834071159363\n",
      "0.36524271965026855\n",
      "0.3453786373138428\n",
      "0.346262127161026\n",
      "0.3535122275352478\n",
      "0.3533371090888977\n",
      "0.372672438621521\n",
      "0.34046047925949097\n",
      "0.3046366572380066\n",
      "0.2900744676589966\n",
      "0.31752949953079224\n",
      "0.34020328521728516\n",
      "0.3270633816719055\n",
      "0.36644768714904785\n",
      "0.38958460092544556\n",
      "0.38811105489730835\n",
      "0.3743719756603241\n",
      "0.32165849208831787\n",
      "0.324491024017334\n",
      "0.2930276393890381\n",
      "0.3855459690093994\n",
      "0.3293301463127136\n",
      "0.35162678360939026\n",
      "0.34240448474884033\n",
      "0.37379327416419983\n",
      "0.3235999345779419\n",
      "0.3330516219139099\n",
      "0.3268912732601166\n",
      "0.3603302240371704\n",
      "0.2761539816856384\n",
      "0.373751699924469\n",
      "0.35770103335380554\n",
      "0.3173863887786865\n",
      "0.3752090334892273\n",
      "0.371404767036438\n",
      "0.36938631534576416\n",
      "0.38305532932281494\n",
      "0.3767179846763611\n",
      "0.3224985897541046\n",
      "0.32298150658607483\n",
      "0.2717868685722351\n",
      "0.3592190742492676\n",
      "0.36545026302337646\n",
      "0.3856184482574463\n",
      "0.31540533900260925\n",
      "0.3622099459171295\n",
      "0.34131860733032227\n",
      "0.32372069358825684\n",
      "0.3881731629371643\n",
      "0.3241649866104126\n",
      "0.33353763818740845\n",
      "0.3246789872646332\n",
      "0.39119404554367065\n",
      "0.3086692690849304\n",
      "0.3193145990371704\n",
      "0.4322623014450073\n",
      "0.34945032000541687\n",
      "0.3317902386188507\n",
      "0.3301538825035095\n",
      "0.30972743034362793\n",
      "0.35644644498825073\n",
      "0.3253512382507324\n",
      "0.35087060928344727\n",
      "0.3914828300476074\n",
      "0.3792751133441925\n",
      "0.38440465927124023\n",
      "0.37992534041404724\n",
      "0.39296406507492065\n",
      "0.33633649349212646\n",
      "0.33261334896087646\n",
      "0.29847562313079834\n",
      "0.2982458472251892\n",
      "0.3196559548377991\n",
      "0.3481771945953369\n",
      "0.37873542308807373\n",
      "0.2758658528327942\n",
      "0.3655819892883301\n",
      "0.3053632974624634\n",
      "0.31703031063079834\n",
      "0.3161545991897583\n",
      "0.3114626407623291\n",
      "0.3590748906135559\n",
      "0.3406299352645874\n",
      "0.3547215461730957\n",
      "0.3313063383102417\n",
      "0.3292991518974304\n",
      "0.2951657176017761\n",
      "0.36068814992904663\n",
      "0.3438190221786499\n",
      "0.3778834939002991\n",
      "0.3116205930709839\n",
      "0.32829540967941284\n",
      "0.3046472668647766\n",
      "2 0.24188540875911713\n",
      "0.35040178894996643\n",
      "0.3394116461277008\n",
      "0.3235386908054352\n",
      "0.3468171954154968\n",
      "0.37122857570648193\n",
      "0.3085528314113617\n",
      "0.31932833790779114\n",
      "0.32636451721191406\n",
      "0.3161136507987976\n",
      "0.30991169810295105\n",
      "0.3827068507671356\n",
      "0.3141133785247803\n",
      "0.30642297863960266\n",
      "0.29107850790023804\n",
      "0.41047587990760803\n",
      "0.32041868567466736\n",
      "0.37736576795578003\n",
      "0.36602020263671875\n",
      "0.29379934072494507\n",
      "0.326180636882782\n",
      "0.337460994720459\n",
      "0.31242018938064575\n",
      "0.3293358385562897\n",
      "0.27912867069244385\n",
      "0.38689300417900085\n",
      "0.29135921597480774\n",
      "0.318597674369812\n",
      "0.3344293236732483\n",
      "0.2886543869972229\n",
      "0.30904221534729004\n",
      "0.3240376114845276\n",
      "0.26112696528434753\n",
      "0.373690664768219\n",
      "0.3857215940952301\n",
      "0.37869876623153687\n",
      "0.37330329418182373\n",
      "0.3446873426437378\n",
      "0.37525489926338196\n",
      "0.326842337846756\n",
      "0.33969932794570923\n",
      "0.33272120356559753\n",
      "0.3189242482185364\n",
      "0.3240942358970642\n",
      "0.3125167191028595\n",
      "0.31292617321014404\n",
      "0.331720232963562\n",
      "0.3362254202365875\n",
      "0.3041347563266754\n",
      "0.29827332496643066\n",
      "0.3551882207393646\n",
      "0.32605910301208496\n",
      "0.3438151478767395\n",
      "0.3485943675041199\n",
      "0.361469566822052\n",
      "0.31043586134910583\n",
      "0.2768104076385498\n",
      "0.3263440430164337\n",
      "0.3662712574005127\n",
      "0.2687075138092041\n",
      "0.3442876935005188\n",
      "0.2983456552028656\n",
      "0.341855525970459\n",
      "0.32780998945236206\n",
      "0.31846922636032104\n",
      "0.3226873278617859\n",
      "0.32205110788345337\n",
      "0.3313499093055725\n",
      "0.3052487075328827\n",
      "0.3216324746608734\n",
      "0.37414538860321045\n",
      "0.3718230724334717\n",
      "0.35253918170928955\n",
      "0.3305369019508362\n",
      "0.27469488978385925\n",
      "0.3390880823135376\n",
      "0.36187076568603516\n",
      "0.26754841208457947\n",
      "0.3482752740383148\n",
      "0.3682614266872406\n",
      "0.3560357689857483\n",
      "0.32375115156173706\n",
      "0.3328215777873993\n",
      "0.32170870900154114\n",
      "0.29654932022094727\n",
      "0.3226078152656555\n",
      "0.32468530535697937\n",
      "0.31253769993782043\n",
      "0.3566124439239502\n",
      "0.33481431007385254\n",
      "0.3353348672389984\n",
      "0.37743479013442993\n",
      "0.3508950471878052\n",
      "0.34851574897766113\n",
      "0.32354438304901123\n",
      "0.3772357702255249\n",
      "0.3053574562072754\n",
      "0.3438819944858551\n",
      "0.31295105814933777\n",
      "0.31397372484207153\n",
      "0.33858659863471985\n",
      "0.3670474886894226\n",
      "0.37356728315353394\n",
      "0.3046664595603943\n",
      "0.2595047950744629\n",
      "0.38145557045936584\n",
      "0.3353661298751831\n",
      "0.301561176776886\n",
      "0.33933812379837036\n",
      "0.3615971803665161\n",
      "0.31525343656539917\n",
      "0.2942877411842346\n",
      "0.2941266596317291\n",
      "0.3090793192386627\n",
      "0.323557585477829\n",
      "0.29906517267227173\n",
      "0.31477493047714233\n",
      "0.3122248947620392\n",
      "0.359239399433136\n",
      "0.301996648311615\n",
      "0.28744739294052124\n",
      "0.31390756368637085\n",
      "0.3121948838233948\n",
      "0.27295807003974915\n",
      "0.33398035168647766\n",
      "0.3673616647720337\n",
      "0.3953443169593811\n",
      "0.33403730392456055\n",
      "0.36131468415260315\n",
      "0.3205340504646301\n",
      "0.3293083906173706\n",
      "0.3217974603176117\n",
      "0.34995341300964355\n",
      "0.4003639221191406\n",
      "0.3394319415092468\n",
      "0.3749629259109497\n",
      "0.34470000863075256\n",
      "0.3195059597492218\n",
      "0.34539133310317993\n",
      "0.2984180748462677\n",
      "0.319669246673584\n",
      "0.3339880704879761\n",
      "0.35506173968315125\n",
      "0.3940514326095581\n",
      "0.27642953395843506\n",
      "0.3073439598083496\n",
      "0.3588488698005676\n",
      "0.3444470763206482\n",
      "0.319793701171875\n",
      "0.3679198622703552\n",
      "0.3386053740978241\n",
      "0.3043419122695923\n",
      "0.33412426710128784\n",
      "0.33809536695480347\n",
      "0.2952266335487366\n",
      "0.31984418630599976\n",
      "0.34394213557243347\n",
      "0.3816843032836914\n",
      "0.2696942090988159\n",
      "0.25064706802368164\n",
      "0.3174746632575989\n",
      "3 0.24021443724632263\n",
      "0.3247475326061249\n",
      "0.2922672927379608\n",
      "0.3388642370700836\n",
      "0.29226648807525635\n",
      "0.2808193564414978\n",
      "0.3066290616989136\n",
      "0.3225669264793396\n",
      "0.3525225520133972\n",
      "0.3090439736843109\n",
      "0.36352604627609253\n",
      "0.31668728590011597\n",
      "0.29823610186576843\n",
      "0.2798542380332947\n",
      "0.38207828998565674\n",
      "0.3015907406806946\n",
      "0.3170393109321594\n",
      "0.2899158000946045\n",
      "0.3253403306007385\n",
      "0.35045912861824036\n",
      "0.3090781569480896\n",
      "0.3338528871536255\n",
      "0.3658023476600647\n",
      "0.32242798805236816\n",
      "0.3367435932159424\n",
      "0.33880263566970825\n",
      "0.29192155599594116\n",
      "0.3142933249473572\n",
      "0.28761160373687744\n",
      "0.26436126232147217\n",
      "0.3537553548812866\n",
      "0.2848106622695923\n",
      "0.33429503440856934\n",
      "0.2642160654067993\n",
      "0.3192664384841919\n",
      "0.36981603503227234\n",
      "0.30561497807502747\n",
      "0.2879254221916199\n",
      "0.33696186542510986\n",
      "0.2972254157066345\n",
      "0.33516591787338257\n",
      "0.3299776613712311\n",
      "0.35076725482940674\n",
      "0.31866490840911865\n",
      "0.3641805946826935\n",
      "0.3496825397014618\n",
      "0.30237460136413574\n",
      "0.42364048957824707\n",
      "0.33760353922843933\n",
      "0.3129402995109558\n",
      "0.3763425350189209\n",
      "0.3302827477455139\n",
      "0.4057576060295105\n",
      "0.3197086453437805\n",
      "0.2823264002799988\n",
      "0.30638301372528076\n",
      "0.30886563658714294\n",
      "0.38843610882759094\n",
      "0.27012819051742554\n",
      "0.31852516531944275\n",
      "0.3327868580818176\n",
      "0.31864166259765625\n",
      "0.28831005096435547\n",
      "0.2912845015525818\n",
      "0.29602277278900146\n",
      "0.34738689661026\n",
      "0.3951079249382019\n",
      "0.31922122836112976\n",
      "0.3268212080001831\n",
      "0.31567028164863586\n",
      "0.3780614137649536\n",
      "0.3115405738353729\n",
      "0.3185953199863434\n",
      "0.2663745880126953\n",
      "0.33857592940330505\n",
      "0.34047073125839233\n",
      "0.35279548168182373\n",
      "0.3278384804725647\n",
      "0.29532432556152344\n",
      "0.32396334409713745\n",
      "0.36595359444618225\n",
      "0.35818547010421753\n",
      "0.25780242681503296\n",
      "0.4015812873840332\n",
      "0.3272120952606201\n",
      "0.34243112802505493\n",
      "0.35359668731689453\n",
      "0.37438467144966125\n",
      "0.27746859192848206\n",
      "0.31884273886680603\n",
      "0.2948429584503174\n",
      "0.3241168260574341\n",
      "0.30715060234069824\n",
      "0.32958483695983887\n",
      "0.3490562438964844\n",
      "0.30814415216445923\n",
      "0.3316839933395386\n",
      "0.3115977644920349\n",
      "0.3017128109931946\n",
      "0.36841878294944763\n",
      "0.3310195207595825\n",
      "0.27912765741348267\n",
      "0.3311672508716583\n",
      "0.32431212067604065\n",
      "0.3046545684337616\n",
      "0.34489554166793823\n",
      "0.3228070139884949\n",
      "0.33266976475715637\n",
      "0.32015615701675415\n",
      "0.3540005683898926\n",
      "0.3290603458881378\n",
      "0.29331353306770325\n",
      "0.29588717222213745\n",
      "0.3393608331680298\n",
      "0.37672343850135803\n",
      "0.25932836532592773\n",
      "0.29339438676834106\n",
      "0.34867674112319946\n",
      "0.286701500415802\n",
      "0.353449285030365\n",
      "0.3087809085845947\n",
      "0.32615751028060913\n",
      "0.2976665794849396\n",
      "0.3575773239135742\n",
      "0.3078957498073578\n",
      "0.2669477164745331\n",
      "0.3087731599807739\n",
      "0.30224090814590454\n",
      "0.3398854732513428\n",
      "0.35309433937072754\n",
      "0.28937453031539917\n",
      "0.29098689556121826\n",
      "0.31359604001045227\n",
      "0.331379234790802\n",
      "0.30370593070983887\n",
      "0.3657163679599762\n",
      "0.30337250232696533\n",
      "0.2721887230873108\n",
      "0.32584285736083984\n",
      "0.3711526393890381\n",
      "0.3225269317626953\n",
      "0.27192312479019165\n",
      "0.361172080039978\n",
      "0.3248467743396759\n",
      "0.32795557379722595\n",
      "0.39699286222457886\n",
      "0.29392296075820923\n",
      "0.3228960335254669\n",
      "0.3798966407775879\n",
      "0.24409928917884827\n",
      "0.34010306000709534\n",
      "0.36893901228904724\n",
      "0.3551689684391022\n",
      "0.29327255487442017\n",
      "0.30089569091796875\n",
      "0.34221214056015015\n",
      "0.3249551057815552\n",
      "0.28469979763031006\n",
      "0.35225021839141846\n",
      "0.3178672194480896\n",
      "0.3645263612270355\n",
      "4 0.475461483001709\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 72.65 GiB (GPU 0; 31.75 GiB total capacity; 20.65 GiB already allocated; 5.02 GiB free; 20.66 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [22], line 85\u001b[0m\n\u001b[1;32m     82\u001b[0m profiler\u001b[38;5;241m.\u001b[39menable()\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# Run the training loop\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# Stop profiling\u001b[39;00m\n\u001b[1;32m     88\u001b[0m profiler\u001b[38;5;241m.\u001b[39mdisable()\n",
      "Cell \u001b[0;32mIn [22], line 52\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(epoch,mse_loss)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m log_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 52\u001b[0m     val_huber,val_mse \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_and_log\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwandb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwandb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43muse_wandb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m     val_mse_loss\u001b[38;5;241m.\u001b[39mappend(val_mse)\n\u001b[1;32m     54\u001b[0m     val_huber_loss\u001b[38;5;241m.\u001b[39mappend(val_huber)\n",
      "Cell \u001b[0;32mIn [15], line 30\u001b[0m, in \u001b[0;36mevaluate_and_log\u001b[0;34m(best_model, x_val, y_val, epoch, wandb)\u001b[0m\n\u001b[1;32m     13\u001b[0m val_pred \u001b[38;5;241m=\u001b[39m (predict_in_batches(best_model, x_val, \u001b[38;5;241m1024\u001b[39m)\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()  \u001b[38;5;66;03m# Adjust batch size as per memory needs\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# print(val_pred==y_val)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# if wandb:\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#     wandb.log({\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m#         \"invalid_mse_loss\": nn.BCELoss()(val_pred[y_val == INVALID_WCD], y_val.view(-1, 1)[y_val == INVALID_WCD]).item(),\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m#     })\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mval_pred\u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;124m\"\u001b[39m,accuracy)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nn\u001b[38;5;241m.\u001b[39mHuberLoss()(val_pred, y_val\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mitem() ,accuracy\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 72.65 GiB (GPU 0; 31.75 GiB total capacity; 20.65 GiB already allocated; 5.02 GiB free; 20.66 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "use_wandb = False\n",
    "if use_wandb:\n",
    "    wandb.init(project='gridworld', save_code=False, config={\"lambda_l2\": lambda_l2, \"grad_clip\":grad_clip,\n",
    "                                                              \"n_train\":x_train.shape[0],\"GRID_SIZE\":GRID_SIZE,\"dropout\":dropout,\n",
    "                                                              \"lr\":lr,\"batch_size\":batch_size, \"experiment\":\"gridworld\"})\n",
    "# create dataset and dataloader\n",
    "# initialize model and optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[12,25,45,55], gamma=0.5)\n",
    "\n",
    "# training loop\n",
    "\n",
    "best_model = model\n",
    "log_interval = 5\n",
    "\n",
    "def train():\n",
    "    training_loss = []\n",
    "    val_mse_loss =[]\n",
    "    val_huber_loss = []\n",
    "    x_epochs = []\n",
    "    \n",
    "    lowest_loss = torch.inf\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (inputs, targets) in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass\n",
    "            outputs = model(inputs)\n",
    "            # targets = targets.cuda()\n",
    "\n",
    "            # compute loss and perform backpropagation\n",
    "            y_true = targets.view(-1, 1)\n",
    "            loss = nn.BCELoss()(outputs, targets.view(-1, 1))\n",
    "            mse_loss = loss.item()\n",
    "            \n",
    "            # l2_reg = lambda_l2 * torch.norm(torch.cat([p.view(-1) for p in model.parameters()]), p=2)  # L2 regularization term\n",
    "            # loss += l2_reg\n",
    "            \n",
    "            loss.backward()\n",
    "            utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            optimizer.step()\n",
    "            if loss < lowest_loss:\n",
    "                lowest_loss = loss.item()\n",
    "                best_model = model\n",
    "            \n",
    "            if (i + 1) % 100*log_interval == 0:\n",
    "                if use_wandb:\n",
    "                    wandb.log({\"loss\": mse_loss})\n",
    "                print(mse_loss)\n",
    "                \n",
    "        print(epoch,mse_loss)\n",
    "        if (epoch + 1) % log_interval == 0:\n",
    "            val_huber,val_mse = evaluate_and_log(best_model, x_val, y_val, epoch, wandb=wandb if use_wandb else None )\n",
    "            val_mse_loss.append(val_mse)\n",
    "            val_huber_loss.append(val_huber)\n",
    "            training_loss.append(mse_loss)\n",
    "            x_epochs.append(epoch)\n",
    "            \n",
    "            plt.plot(x_epochs,training_loss, label=\"Training\")\n",
    "            plt.plot(x_epochs,val_mse_loss, label=\"Validation\")\n",
    "            plt.show()\n",
    "            \n",
    "        scheduler.step()\n",
    "        torch.save(best_model, f\"models/valid_wcd_model_{GRID_SIZE}.pt\")\n",
    "    plt.plot(x_epochs,training_loss, label=\"Training\")\n",
    "    plt.plot(x_epochs,val_mse_loss, label=\"Validation\")\n",
    "\n",
    "    # Adding labels\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training and Validation MSE Loss\")\n",
    "\n",
    "    # Setting y-axis limits and grid lines in intervals of 5\n",
    "    # plt.ylim(0, 30)\n",
    "    # plt.yticks(range(0, 21, 5))\n",
    "    plt.grid(axis='y')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "print('Starting Training')\n",
    "# Start profiling\n",
    "profiler = cProfile.Profile()\n",
    "profiler.enable()\n",
    "\n",
    "# Run the training loop\n",
    "train()\n",
    "\n",
    "# Stop profiling\n",
    "profiler.disable()\n",
    "# profiler.print_stats()\n",
    "\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560007ba-9f57-4c84-82a3-59fb52b0f837",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647a0d3a-7821-4e2e-96d3-0cf84ad3b6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.HuberLoss()(best_model(x_test),y_test.view(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55be394c-7bfa-4d36-9000-fcc86afbe96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean(abs(best_model(x_test)-y_test.view(-1, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73be668d-a11e-40da-9560-025e3795c29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean(abs(best_model(x_test)-y_test.view(-1, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389a1bca-e16a-406d-bb4b-8fc66a7a47a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(best_model(x_test).cpu().detach().numpy(), fill=True,label=\"Pred\")\n",
    "sns.kdeplot(y_test.view(-1, 1).cpu().detach().numpy(), fill=True,label=\"True\",color=\"green\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622ac26a-0439-43d7-8aa1-7ef0a1cc9b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "(best_model(x_test).cpu().detach().numpy()<0).sum()\n",
    "(y_test<0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e5d5c9-d3d1-4c79-9936-328fb14656e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best_model,f\"models/wcd_nn_oracle_{GRID_SIZE}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b87779-e16b-4fbe-a328-79525a6122f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db961192-6d58-440c-84bd-7ebab2a7e25b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae488dcf-2114-40f7-8e38-5336144ea526",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8080ebf5-5808-4489-94a9-c1bf70506524",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a833a8a-588b-4db8-8cd3-b702d34aac8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
